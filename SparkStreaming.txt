1）CentOS7 软件安装以及环境配置
1.1 使用普通用户（hadoop）进行解压并移动到指定文件目录下 sudo tar -zxvf *.tar.gz -C ../app

2)shell bash 测试命令写入文件时候
echo text >> data.log

3)启动flume agent时候使用到log4j 无法扫描本地计算机绝对路径
错误信息：log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: ./logs/flume.log (No such file or directory) 
3.1） 查看文件是否存在并付给文件读写执行权限。
3.2) 注意编写时连接符号 \一定要前后打空格

4）zookeeper启动报错 日志信息存在bin目录下
4.1 ： 在zkServer.sh 脚本上加入 JAVA环境变量


exec-memory-avro.conf

# 配置组件名称
exec-memory-avro.sources = exec-source
exec-memory-avro.sinks = avro-sink
exec-memory-avro.channels = memory-channel

# 配置source组件
exec-memory-avro.sources.exec-source.type = exec
exec-memory-avro.sources.exec-source.command = tail -F /home/hadoop/data/data.log
exec-memory-avro.sources.exec-source.shell = /bin/sh -c

# 配置sink组件
exec-memory-avro.sinks.avro-sink.type = avro
exec-memory-avro.sinks.avro-sink.hostname = hadoop001
exec-memory-avro.sinks.avro-sink.port = 44444

# 配置channel组件
exec-memory-avro.channels.memory-channel.type = memory

# 配置组件之间连接关系
exec-memory-avro.sources.exec-source.channels = memory-channel
exec-memory-avro.sinks.avro-sink.channel = memory-channel

-----------------------------------------------------------
avro-memory-logger.conf

avro-memory-logger.sources = avro-source
avro-memory-logger.sinks = logger-sink
avro-memory-logger.channels = memory-channel

avro-memory-logger.sources.avro-source.type = avro
avro-memory-logger.sources.avro-source.bind = hadoop001
avro-memory-logger.sources.avro-source.port = 44444
 
avro-memory-logger.sinks.logger-sink.type = logger

avro-memory-logger.channels.memory-channel.type = memory

avro-memory-logger.sources.avro-source.channels = memory-channel
avro-memory-logger.sinks.logger-sink.channel = memory-channel


先启动avro-memory-logger
$ bin/flume-ng agent \
--name avro-memory-logger \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/avro-memory-logger.conf \
-Dflume.root.logger=INFO,console

$ bin/flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/exec-memory-avro.conf \
--name exec-memory-avro \
-Dflume.root.logger=INFO,console


单节点单broker搭建：
$KAFKA_HOME/conf
1）配置server.properties 
broker.id=0
host.name=hadoop001
log.dirs=/home/hadoop/app/tmp/kafka-logs
zookeeper.connect=hadoop001:2181

2）启动kafka
kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties

3) 创建topic
kafka-topics.sh --create --zookeeper hadoop001:2181  --replication-factor 1 --partitions 1 --topic hello_topic
分区数1 副本数1

4)查看所有topic
kafka-topics.sh --list --zookeeper hadoop001:2181

5)创建console生产者
kafka-console-producer.sh --broker-list hadoop001:9092 --topic hello_topic

6)创建console消费者
kafka-console-consumer.sh --zookeeper hadoop001:2181 --topic hello_topic --from-beginning

--from-beginning使用
使用from-beging会将之前生成的消息加载
 
7）查看所有toppic的详细信息
kafka-topics.sh --describe --zookeeper hadoop001:2181 
7.1 查看指定topic
--topic hello_topic


单节点多borker搭建:
1. log.dirs
2. listeners
3. borker.id
server-1.properties
server-2.properties
server-3.properties
1)启动
kafka-server-start.sh -daemon $KAFKA_HOME/config/server-1.properties
kafka-server-start.sh -daemon $KAFKA_HOME/config/server-2.properties
kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties

2)创建topic
kafka-topics.sh --create --zookeeper hadoop001:2181 --replication-factor 3 --partitions 1 --topic trafficSpatiotempora_topic

3)生产者
kafka-console-producer.sh --broker-list hadoop001:9093,hadoop001:9094,hadoop001:9095 --topic my-replicated-topic

4）消费者
kafka-console-consumer.sh --zookeeper hadoop001:2181 --topic my-replicated-topic

kafka的容错性测试！ 

KafkaAPI测试
1）kafka配置文件中添加
advertised.host.name=192.168.10.133
advertised.port=+9092


Flume对接Kafka
修改avro-memory-logger sink对接Kafka

avro-memory-kafka.conf
avro-memory-kafka.sources = avro-source
avro-memory-kafka.sinks = kafka-sink
avro-memory-kafka.channels = memory-channel

avro-memory-kafka.sources.avro-source.type = avro
avro-memory-kafka.sources.avro-source.bind = hadoop001
avro-memory-kafka.sources.avro-source.port = 44444
 
avro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
avro-memory-kafka.sinks.kafka-sink.brokerList = hadoop001:9092
avro-memory-kafka.sinks.kafka-sink.topic = hello_topic
# 需要等待5条生产数据才能消费
avro-memory-kafka.sinks.kafka-sink.batchSize = 5
avro-memory-kafka.sinks.kafka-sink.requiredAcks = 1

avro-memory-kafka.channels.memory-channel.type = memory

avro-memory-kafka.sources.avro-source.channels = memory-channel
avro-memory-kafka.sinks.kafka-sink.channel = memory-channel

启动：
$ bin/flume-ng agent \
--name avro-memory-kafka \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/avro-memory-kafka.conf \
-Dflume.root.logger=INFO,console


$ bin/flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/exec-memory-avro.conf \
--name exec-memory-avro \
-Dflume.root.logger=INFO,console


hadoop配置
1. hadoop-env.sh
2. core-site.xml
3. hdfs-site.xml
4. slaves

./hdfs namenode -format 
start-dfs.sh

5. mapred-site.xml.template
6. yarn-site.xml

hbase
1 habse-env.xml
export JAVA_HOME
export HBASE_MANAGES_ZK=false

2.  hbase-site.xml
设置分布式
hbase.cluster.distributed

3. regionservers

启动
start-hbase.sh 


spark编译成生成环境上对应Hadoop版本的安装包

./dev/make-distribution.sh \
--name custom-spark 
--tgz \
-Psparkr \
-Phadoop-2.6 \
-Phive \
-Phive-thriftserver \
-Pmesos \
-Pyarn \

./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0

./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz -Dhadoop.version=2.6.0-cdh5.7.0 -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Pkubernetes

mvn -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package 


Spark Streaming
Spark Streaming is an extension of the core Spark API that enables scalable,
high-throughput,
fault-tolerant 
stream processing of live data streams.

  Spark Streaming ：将不同的数据源经过Spark Streaming处理过后将结果输出到文件系统中。

  特点：
   		低延迟
   		能从错误运行中高效恢复
   		能够运行在成百上千的节点
   		能将批处理、机器学习、图计算等子框架和Spark Streaming综合使用



Spark Streaming是否需要独立安装？

One stack to rule them all： 一栈式


https://github.com/apache/spark

1） Spark-submit

./spark-submit --master local[2] \
--class org.apache.spark.examples.streaming.NetworkWordCount \
--name NetworkWordCount \
/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.2.0.jar hadoop000 9999

2） Spark-shell
./spark-shell --master local[2]

import org.apache.spark.streaming.{Seconds, StreamingContext}

val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream("hadoop000", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()

粗粒度：

工作原理：Spakr Streaming接受到实时数据流，把数据按照指定的时间段切成一片一片的小块数据块，然后把小块的数据传给Spark Engine处理

细粒度：



SparkStreaming核心概念

1）StreamingContext
  def this(sparkContext: SparkContext, batchDuration: Duration) = {
    this(sparkContext, null, batchDuration)
  }


2）Discretized Stream
代表一系列的RDD

对DStream操作算子，比如map/flatMap。其实底层会被翻译成对DStream中每一个RDD做相同的操作。因为一个DStream是由不同批次的RDD所构成的。

3）Input DStream and Receivers
Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.

4） Transformations 

5） Output Operations

updateStateByKey算子
统计在目前为止累积出现的单词个数（需要保存之前的状态）
requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().

window：

window length ：窗口的长度
sliding interval： 窗口的间隔

batch size：倍数
每隔多久计算某一个方范围内的数据


黑名单过滤:

20180410,zs
20180810,ls
20180410,ww

 val lines: ReceiverInputDStream[String] = ssc.socketTextStream("192.168.10.133", 6789)

val clickLog = lines.map(x => (x.split(",")(1), x)).transform(rdd => {
  rdd.leftOuterJoin(blacksRDD)
    .filter(x => !x._2._2.getOrElse(false))
    .map(x => (x._2._1))
})


SparkStreaming整合FLume

第一种方式（Push）：

Agent编写： flume_push_streaming.conf

# 配置组件名称
simple-agent.sources = netcat-source
simple-agent.sinks = avro-sink
simple-agent.channels = memory-channel

# 配置source组件
simple-agent.sources.netcat-source.type = netcat
simple-agent.sources.netcat-source.bind = 192.168.0.108
simple-agent.sources.netcat-source.port = 44444

# 配置sink组件
simple-agent.sinks.avro-sink.type = avro
simple-agent.sinks.avro-sink.hostname = hadoop001
simple-agent.sinks.avro-sink.port = 41414

# 配置channel组件
simple-agent.channels.memory-channel.type = memory

# 配置组件之间连接关系
simple-agent.sources.netcat-source.channels = memory-channel
simple-agent.sinks.avro-sink.channel = memory-channel


flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/flume_push_streaming.conf \
--name simple-agent \
-Dflume.root.logger=INFO,console

1）启动SpakrStreaming作业
2）启动flume agent
3）使用telent 向41414 端口发送消息 观察idea控制台


idea maven项目打jar包命令
mvn clean package -DskipTests


生产环境测试
 ./bin/spark-submit \
--packages org.apache.spark:spark-streaming-flume_2.11:2.2.0 \
--class com.fengli.spark.FlumePushWordCount \
--master local[2] \
/home/hadoop/lib/spakTrainTest-1.0.jar \
hadoop001 41414


SparkStreaming整合Flume Pull方式
Approach 2: Pull-based Approach using a Custom Sink


Flume Agent

flume_pull_streaming.conf

# 配置组件名称
simple-agent.sources = netcat-source
simple-agent.sinks = spark-sink
simple-agent.channels = memory-channel

# 配置source组件
simple-agent.sources.netcat-source.type = netcat
simple-agent.sources.netcat-source.bind = hadoop001
simple-agent.sources.netcat-source.port = 44444
 
# 配置sink组件
simple-agent.sinks.spark-sink.type = org.apache.spark.streaming.flume.sink.SparkSink
simple-agent.sinks.spark-sink.hostname = hadoop001
simple-agent.sinks.spark-sink.port = 41414

# 配置channel组件
simple-agent.channels.memory-channel.type = memory

# 配置组件之间连接关系
simple-agent.sources.netcat-source.channels = memory-channel
simple-agent.sinks.spark-sink.channel = memory-channel

flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/flume_pull_streaming.conf \
--name simple-agent \
-Dflume.root.logger=INFO,console


spark-submit \
--packages org.apache.spark:spark-streaming-flume_2.11:2.2.0 \
--class com.fengli.spark.FlumePullWordCount \
--master local[2] \
/home/hadoop/lib/spakTrainTest-1.0_pull.jar \
hadoop001 41414



Spark Streaming整合kafka

 Spark核心功能 ：Write Ahead Logs（Received） 、Direct 模式、

 1）Receiver-based Approach
zookeeper启动
kafka启动
创建topic
kafka-topics.sh --create --zookeeper hadoop001:2181 --replication-factor 1 --partitions 1 --topic kafka_streaming_topic 
通过控制台测试topic是否能正常生产消费

mvn clean package -DskipTests

spark-submit \
--class com.fengli.spark.KafkaDirectWordCount \
--jars /home/hadoop/lib/spark-streaming-kafka-0-8-assembly_2.11-2.4.4.jar \
--master local[2] \
/home/hadoop/lib/sparktrain-1.0_Direct_kafka.jar \
192.168.10.133:9092 kafka_streaming_topic


SparkStreaming整合Flum + Kafka


streaming.conf

agent1.sources=avro-source
agent1.channels=logger-channel
agent1.sinks=log-sink

# define source
agent1.sources.avro-source.type=avro
agent1.sources.avro-source.bind=0.0.0.0
agent1.sources.avro-source.port=41414

# define channel
agent1.channels.logger-channel.type=memory

# define sink
agent1.sinks.log-sink.type=logger

agent1.sources.avro-source.channels=logger-channel
agent1.sinks.log-sink.channel=logger-channel

flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/streaming.conf \
--name agent1 \
-Dflume.root.logger=DEBUG,console


kafka-topics.sh --create --zookeeper hadoop001:2181 --replication-factor 1 --partitions 1 --topic streaming_topic

------------------------------------------
【log4j_flume_kafka.conf】

agent1.sources=avro-source
agent1.channels=logger-channel
agent1.sinks=kafka-sink

# define source
agent1.sources.avro-source.type=avro
agent1.sources.avro-source.bind=0.0.0.0
agent1.sources.avro-source.port=41414

# define channel
agent1.channels.logger-channel.type=memory

# define sink
agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink
agent1.sinks.kafka-sink.requiredAcks=1
agent1.sinks.kafka-sink.topic=streaming_topic
agent1.sinks.kafka-sink.batchSize=20
agent1.sinks.kafka-sink.brokerList=hadoop001:9092

agent1.sources.avro-source.channels=logger-channel
agent1.sinks.kafka-sink.channel=logger-channel

flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/log4j_flume_kafka.conf \
--name agent1 \
-Dflume.root.logger=DEBUG,console

kafka-console-consumer.sh --zookeeper hadoop001:2181 --topic streaming_topic

